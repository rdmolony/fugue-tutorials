{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputter\n",
    "\n",
    "`Outputter` represents a terminal piece of logic in a workflow. Outputter is the only Fugue extension that does not return a DataFrame. It is called Outputter because it is normally used to save data or print on the console. `Outputter` is used on entire dataframes and executes on the driver. Fugue's `save` is an example of an Outputter\n",
    "\n",
    "In this tutorial are the methods to define an `Outputter`. There is no preferred method and Fugue makes it flexible for users to choose whatever interface works for them. The four ways are native approach, schema hint, decorator, and the class interface in order of simplicity.\n",
    "\n",
    "## Example Use Cases\n",
    "\n",
    "* **Pretty printers for console and Jupyter**\n",
    "* **Writing data to a database**\n",
    "* **Unit test assertions** can be done by taking in a DataFrame and checking the values.\n",
    "\n",
    "## Quick Notes on Usage\n",
    "\n",
    "**ExecutionEngine aware**\n",
    "\n",
    "* `Outputters` run on the driver so they are aware of the `ExecutionEngine` being used. Passing a parameter with the `ExecutionEngine` annotation will pass in the current `ExecutionEngine`. There is an example of this later.\n",
    "\n",
    "**Acceptable input DataFrame types**\n",
    "\n",
    "* `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "* Input can also be Fugue `DataFrames`, which is a collection of Fugue `DataFrame`. \n",
    "\n",
    "**Acceptable output DataFrame types**\n",
    "\n",
    "* `Outputter` can't output anything. The annotation has to be `None`.\n",
    "\n",
    "**Further notes**\n",
    "\n",
    "* `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame` or `DataFrame`\n",
    "* Variations of `LocalDataFrame` will bring the entire dataset onto driver, for an Outputter this might be an expected operation, but you need to be careful.\n",
    "* `Iterable`-like input may have different exeuction plans to bring data to driver, in some cases it can be less optimial (slower), you need to be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Approach\n",
    "\n",
    "The native approach is using a regular function without any edits beyond type annotations. You just need to have acceptable type annotations for the input DataFrames and the output annotation should be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 2], [1, 3], [1, 1]]\n",
      "[[0, 1], [0, 2], [1, 3], [1, 1]]\n",
      "   a  b\n",
      "0  0  1\n",
      "1  0  2\n",
      "2  1  3\n",
      "3  1  1\n",
      "[[0, 1], [0, 2], [1, 3], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "def out(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df)\n",
    "\n",
    "def out2(df1:pd.DataFrame, df2:List[List[Any]]) -> None:\n",
    "    print(df1)\n",
    "    print(df2)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.output(out, params={\"n\":2})\n",
    "    dag.output(df,df,using=out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important to know how to use `DataFrames` as input annotation. Because this is the only way accept a **dynamic** number of input DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_0\n",
      "ArrayDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |1    \n",
      "Total count: 1\n",
      "\n",
      "_1\n",
      "ArrayDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |2    \n",
      "1    |3    \n",
      "Total count: 2\n",
      "\n",
      "_2\n",
      "ArrayDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |1    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import DataFrames\n",
    "\n",
    "def out(dfs:DataFrames) -> None:\n",
    "    for k, v in dfs.items():\n",
    "        v.show(title=k)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,2],[1,3]],\"a:int,b:int\")\n",
    "    df3 = dag.df([[1,1]],\"a:int,b:int\")\n",
    "    dag.output(df1,df2,df3,using=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Hint\n",
    "\n",
    "The schema hint does not apply to the output of `Outputter` because the output annotation has to be None and there is no DataFrame returned. A schema hint with `schema: None` can be used but it does not do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "from fugue import outputter\n",
    "\n",
    "# schema: None\n",
    "def out(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "Similar to the schema hint, there is no obvious advantage to use decorator for `Outputter` because there is no output schema so the decorator doesn't do much besides making the code more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "from fugue import outputter\n",
    "\n",
    "@outputter()\n",
    "def out(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach (Advanced)\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But if you want to get all execution context such as partition information, use interface.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import Outputter\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "class Save(Outputter):\n",
    "    def process(self, dfs:DataFrames) -> None:\n",
    "        assert len(dfs)==1\n",
    "        assert isinstance(self.execution_engine, SparkExecutionEngine)\n",
    "        session = self.execution_engine.spark_session\n",
    "        # we get the partition information from Outputter\n",
    "        by = self.partition_spec.partition_by\n",
    "        df = self.execution_engine.to_df(dfs[0])\n",
    "        path = self.params.get_or_throw(\"path\",str)\n",
    "        df.native.write.partitionBy(*by).format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,3],[1,2],[1,1]],\"a:int,b:int\")\n",
    "    df.partition(by=[\"a\"]).output(Save, params=dict(path=\"/tmp/x.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ExecutionEngine\n",
    "\n",
    "In some cases, the `Outputter` has to be aware of the `ExecutionEngine`. **This is an example of how to write native Spark code inside Fugue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  0|  2|\n",
      "|  1|  3|\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "\n",
    "# pay attention to the input annotations\n",
    "def out(e:ExecutionEngine, df:DataFrame) -> None:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    df = e.to_df(df) # to make sure df is Spark DataFrame, or conversion is done here\n",
    "    df.native.show()\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.output(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}