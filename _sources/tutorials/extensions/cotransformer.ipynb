{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CoTransformer\n",
    "\n",
    "`CoTransformer` represents the logic unit executing on arbitrary machine on a collection of partitions of the same partition keys of the input dataframes. The partitioning logic is not a concern of `CoTransformer`, it must be specified by `zip` in the previous step. You must understand [partition](../advanced/partition.ipynb) and [zip](../advanced/execution_engine.ipynb#Zip-&-Comap)\n",
    "\n",
    "In this tutorial are the methods to define an `CoTransformer`. There is no preferred method and Fugue makes it flexible for users to choose whatever interface works for them. The four ways are native approach, schema hint, decorator, and the class interface in order of simplicity. \n",
    "\n",
    "## Example Use Cases\n",
    "\n",
    "`CoTransformer` deals with multiple DataFrames partitioned in the same way and returns one DataFrame. So in a lot of cases, it will be used for joins that are traditionally hard to do.\n",
    "\n",
    "* **As-of merge for multiple tables**\n",
    "* **Using serialized objects - model prediction per partition**\n",
    "\n",
    "## Quick Notes on Usage\n",
    "\n",
    "**ExecutionEngine unaware**\n",
    "\n",
    "* `Transformers` are executed on the workers, meaning that they are not unaware of the `ExecutionEngine`.\n",
    "\n",
    "**Acceptable input DataFrame types**\n",
    "\n",
    "* `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "* Can also be a single `DataFrames`\n",
    "\n",
    "**Acceptable output DataFrame types** \n",
    "\n",
    "* `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Explicit output schema**\n",
    "\n",
    "* `CoTransformer` requires users to be explicit on the output schema. Different from `Transformer`, `*` is not allowed because there is a chance for column names to collide.\n",
    "* Notice that `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Scenario - Model Predictions per Partition\n",
    "\n",
    "`CoTransformer` is a bit hard to conceptually understand at first, so we need to clarify the example in this tutorial. First, we'll create a DataFrame with groups A,B, and C. For each of these groups, we have 2 features in `x1` and `x2`. The target variable will be `y`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "data = pd.DataFrame({\"group\": ([\"A\"]*5 + [\"B\"]*5 + [\"C\"]*5),\n",
    "                    \"x1\": np.random.normal(0,1,15).round(3),\n",
    "                    \"x2\": np.random.normal(0,1,15).round(3),\n",
    "                    \"y\": np.random.normal(0,1,15).round(3)})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we make two functions. First will be `transformer` named `train_model` to train a model for each group and return it in serialized form. The serialized form is needed because Spark DataFrames can't hold Python classes unless they are serialized. Note that by the time the data enters `train_model`, it is already partitioned.\n",
    "\n",
    "The second function is a `predict` function that will take in two DataFrames. The first will be named `data` and the second will be `models`. By the time these DataFrames come into the function, they will already have been paritioned to work on a logical group. Because we only have one model per group, `models` will actually be a DataFrame that only contains one row. It becomes really easy to access the model if we use the `List[Dict[str,Any]]` annotation. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from typing import Dict, Any, List\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "\n",
    "# schema: group:str, model:binary\n",
    "def train_model(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    # the value of group will be the same because it's partitioned\n",
    "    group = df[\"group\"].iloc[0]\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[[\"x1\", \"x2\"]], df[\"y\"])\n",
    "\n",
    "    # we return the group and the model\n",
    "    return [[group, pickle.dumps(model)]]\n",
    "\n",
    "#schema: group:str, x1:double, x2:double, y_pred:double\n",
    "def predict(data:pd.DataFrame, models:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    model = pickle.loads(models[0]['model'])\n",
    "    data['y_pred'] = model.predict(data[[\"x1\", \"x2\"]])\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that in order to pair the correct model with the appropriate group of data (A,B,C), we need to partition both DataFrames in the same way (by `group`). This is what a `CoTransformer` is meant for, operating of multiple DataFrames partitioned the same way. To demo this, we'll just apply the models on the original dataset (not advised for production but illustrative for this demo).\n",
    "\n",
    "Before we do that, we'll just show what the model table will look like with one model for each group."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    train = dag.df(data)                # original pd.DataFrame\n",
    "    models = train.partition_by(\"group\").transform(train_model)\n",
    "    models.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "group:str|model:bytes                                                                               \n",
      "---------+------------------------------------------------------------------------------------------\n",
      "A        |b'\\x80\\x03csklearn.linear_model._base\\nLinearRegression\\nq\\x00)\\x81q\\x01}q\\x02(X\\r\\x00\\x00\n",
      "         |\\x00fit_interceptq\\x03\\x88X\\t\\x00\\x00\\x00normalizeq\\x04\\x89X\\x06\\x00\\x00\\x00copy_Xq\\x05\\x8\n",
      "         |8X\\x06\\x00\\x00\\x00n_jobsq\\x06NX\\x08\\x00\\x00\\x00positiveq\\x07\\x89X\\x0e\\x00\\x00\\x00n_feature\n",
      "         |s_in_q\\x08K\\x02X\\x05\\x00\\x00\\x00coef_q\\tcnumpy.core.multiarray\\n_reconstruct\\nq\\ncnumpy\\nn\n",
      "         |darray\\nq\\x0bK\\x00\\x85q\\x0cC\\x01bq\\r\\x87q\\x0eRq\\x0f(K\\x01K\\x02\\x85q\\x10cnumpy\\ndtype\\nq\\x1\n",
      "         |1X\\x02\\x00\\x00\\x00f8q\\x12\\x89\\x88\\x87q\\x13Rq\\x1...                                        \n",
      "B        |b'\\x80\\x03csklearn.linear_model._base\\nLinearRegression\\nq\\x00)\\x81q\\x01}q\\x02(X\\r\\x00\\x00\n",
      "         |\\x00fit_interceptq\\x03\\x88X\\t\\x00\\x00\\x00normalizeq\\x04\\x89X\\x06\\x00\\x00\\x00copy_Xq\\x05\\x8\n",
      "         |8X\\x06\\x00\\x00\\x00n_jobsq\\x06NX\\x08\\x00\\x00\\x00positiveq\\x07\\x89X\\x0e\\x00\\x00\\x00n_feature\n",
      "         |s_in_q\\x08K\\x02X\\x05\\x00\\x00\\x00coef_q\\tcnumpy.core.multiarray\\n_reconstruct\\nq\\ncnumpy\\nn\n",
      "         |darray\\nq\\x0bK\\x00\\x85q\\x0cC\\x01bq\\r\\x87q\\x0eRq\\x0f(K\\x01K\\x02\\x85q\\x10cnumpy\\ndtype\\nq\\x1\n",
      "         |1X\\x02\\x00\\x00\\x00f8q\\x12\\x89\\x88\\x87q\\x13Rq\\x1...                                        \n",
      "C        |b'\\x80\\x03csklearn.linear_model._base\\nLinearRegression\\nq\\x00)\\x81q\\x01}q\\x02(X\\r\\x00\\x00\n",
      "         |\\x00fit_interceptq\\x03\\x88X\\t\\x00\\x00\\x00normalizeq\\x04\\x89X\\x06\\x00\\x00\\x00copy_Xq\\x05\\x8\n",
      "         |8X\\x06\\x00\\x00\\x00n_jobsq\\x06NX\\x08\\x00\\x00\\x00positiveq\\x07\\x89X\\x0e\\x00\\x00\\x00n_feature\n",
      "         |s_in_q\\x08K\\x02X\\x05\\x00\\x00\\x00coef_q\\tcnumpy.core.multiarray\\n_reconstruct\\nq\\ncnumpy\\nn\n",
      "         |darray\\nq\\x0bK\\x00\\x85q\\x0cC\\x01bq\\r\\x87q\\x0eRq\\x0f(K\\x01K\\x02\\x85q\\x10cnumpy\\ndtype\\nq\\x1\n",
      "         |1X\\x02\\x00\\x00\\x00f8q\\x12\\x89\\x88\\x87q\\x13Rq\\x1...                                        \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Native Approach\n",
    "\n",
    "The simplest way, with no dependency on Fugue. You just need to have acceptable annotations on input dataframes and output. In native approach, you must specify schema in the Fugue code. We will define the `predict` function again with no annotations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "def predict(data:pd.DataFrame, models:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    model = pickle.loads(models[0]['model'])\n",
    "    data['y_pred'] = model.predict(data[[\"x1\", \"x2\"]])\n",
    "    return data\n",
    "\n",
    "out_schema = \"group:str, x1:double, x2:double, y_pred:double\"\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    train = dag.df(data)                # original pd.DataFrame\n",
    "    models = train.partition_by(\"group\").transform(train_model)\n",
    "\n",
    "    pred = train[[\"group\", \"x1\", \"x2\"]] # simulated test data\n",
    "\n",
    "    # specifying partition and applying the cotransformer\n",
    "    pred.zip(models, partition={\"by\":\"group\"}).transform(predict, schema=out_schema).show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "group:str|x1:double|x2:double|y_pred:double                                                         \n",
      "---------+---------+---------+----------------------------------------------------------------------\n",
      "A        |1.764    |0.334    |-0.7738478452015277                                                   \n",
      "A        |0.4      |1.494    |0.22878910751099973                                                   \n",
      "A        |0.979    |-0.205   |-0.17631594944134255                                                  \n",
      "A        |2.241    |0.313    |-1.1310124534267185                                                   \n",
      "A        |1.868    |-0.854   |-0.8316128594414108                                                   \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Schema Hint\n",
    "\n",
    "The schema can also be provided during the function definition through the use of the schema hint comment. Providing it during definition means it does not need to be provided inside the `FugueWorkflow`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#schema: group:str, x1:double, x2:double, y_pred:double\n",
    "def predict(data:pd.DataFrame, models:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    model = pickle.loads(models[0]['model'])\n",
    "    data['y_pred'] = model.predict(data[[\"x1\", \"x2\"]])\n",
    "    return data\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    train = dag.df(data)                # original pd.DataFrame\n",
    "    models = train.partition_by(\"group\").transform(train_model)\n",
    "\n",
    "    pred = train[[\"group\", \"x1\", \"x2\"]] # simulated test data\n",
    "    pred.zip(models, partition={\"by\":\"group\"}).transform(predict).show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "group:str|x1:double|x2:double|y_pred:double                                                         \n",
      "---------+---------+---------+----------------------------------------------------------------------\n",
      "A        |1.764    |0.334    |-0.7738478452015277                                                   \n",
      "A        |0.4      |1.494    |0.22878910751099973                                                   \n",
      "A        |0.979    |-0.205   |-0.17631594944134255                                                  \n",
      "A        |2.241    |0.313    |-1.1310124534267185                                                   \n",
      "A        |1.868    |-0.854   |-0.8316128594414108                                                   \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decorator Approach\n",
    "\n",
    "The decorator approach also has the special schema syntax and it can also take a function that generates the schema. This can be used to create new column names or types based on cotransformer parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from fugue import FugueWorkflow, Schema, cotransformer\n",
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "    \n",
    "@cotransformer(\"group:str, x1:double, x2:double, y_pred:double\")\n",
    "def predict(data:pd.DataFrame, models:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    model = pickle.loads(models[0]['model'])\n",
    "    data['y_pred'] = model.predict(data[[\"x1\", \"x2\"]])\n",
    "    return data\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    train = dag.df(data)                # original pd.DataFrame\n",
    "    models = train.partition_by(\"group\").transform(train_model)\n",
    "\n",
    "    pred = train[[\"group\", \"x1\", \"x2\"]] # simulated test data\n",
    "    pred.zip(models, partition={\"by\":\"group\"}).transform(predict).show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "group:str|x1:double|x2:double|y_pred:double                                                         \n",
      "---------+---------+---------+----------------------------------------------------------------------\n",
      "A        |1.764    |0.334    |-0.7738478452015277                                                   \n",
      "A        |0.4      |1.494    |0.22878910751099973                                                   \n",
      "A        |0.979    |-0.205   |-0.17631594944134255                                                  \n",
      "A        |2.241    |0.313    |-1.1310124534267185                                                   \n",
      "A        |1.868    |-0.854   |-0.8316128594414108                                                   \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interface Approach (Advanced)\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But for certain cases, implementing the interface approach significantly improves performance. Example scenarios to use the interface approach are:\n",
    "\n",
    "* The output schema needs partition information, such as partition keys, schema, and current values of the keys.\n",
    "* The transformer has an expensive but common initialization step for processing each logical partition. Initialization should then happen when initialiazing physical partition, meaning it doesn't unnecessarily repeat.\n",
    "\n",
    "The biggest advantage of interface approach is that you can customize physical partition level initialization, and you have all the up-to-date context variables to use. In the interface approach, type annotations are not necessary, but again, it's good practice to have them.\n",
    "\n",
    "This example will be different from the previous ones because it will demonstrate how to use context variables. For this example, we will create a new column with the model name `(model_A, model_B, modelC)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from fugue import CoTransformer, DataFrame, PandasDataFrame\n",
    "from triad.collections import Schema\n",
    "from time import sleep\n",
    "\n",
    "class Predict(CoTransformer):\n",
    "    # this is invoked on driver side\n",
    "    def get_output_schema(self, dfs):\n",
    "        return self.key_schema + \"x1:double, x2:double, y_pred:double, model_name:str\"\n",
    "    \n",
    "    # on initialization of the physical partition\n",
    "    def on_init(self, df: DataFrame) -> None:\n",
    "        sleep(0)\n",
    "        \n",
    "    def transform(self, dfs) -> pd.DataFrame:\n",
    "        data = dfs[0].as_pandas()\n",
    "        models = dfs[1].as_dict_iterable()\n",
    "        model = pickle.loads(next(models)['model'])\n",
    "        data['y_pred'] = model.predict(data[[\"x1\", \"x2\"]])\n",
    "        data['model_name'] = \"model_\"+self.cursor.key_value_array[0]\n",
    "        return PandasDataFrame(data)\n",
    "        \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    train = dag.df(data)                # original pd.DataFrame\n",
    "    models = train.partition_by(\"group\").transform(train_model)\n",
    "    pred = train[[\"group\", \"x1\", \"x2\"]] # simulated test data\n",
    "    dag.zip(dict(data=pred,models=models), partition={\"by\":\"group\"}).transform(Predict).show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "group:str|x1:double|x2:double|y_pred:double                                          |model_name:str\n",
      "---------+---------+---------+-------------------------------------------------------+--------------\n",
      "A        |1.764    |0.334    |-0.7738478452015277                                    |model_A       \n",
      "A        |0.4      |1.494    |0.22878910751099973                                    |model_A       \n",
      "A        |0.979    |-0.205   |-0.17631594944134255                                   |model_A       \n",
      "A        |2.241    |0.313    |-1.1310124534267185                                    |model_A       \n",
      "A        |1.868    |-0.854   |-0.8316128594414108                                    |model_A       \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice a few things here:\n",
    "\n",
    "* How we access the key schema (`self.key_schema`), and current logical partition's keys as array (`self.cursor.key_value_array`)\n",
    "* Although DataFrames is a dict, it's an ordered dict following the input order, so you can iterate in this way\n",
    "* `expensive_init` is something that is a common initialization for different logical partitions, we move it to `on_init` so it will run once for each physcial partition."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using DataFrames\n",
    "\n",
    "Instead of using dataframes as input, you can use a single `DataFrames` for arbitrary number of inputs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import DataFrames, FugueWorkflow\n",
    "\n",
    "#schema: res:[str]\n",
    "def to_str(dfs:DataFrames) -> List[List[Any]]:\n",
    "    return [[[x.as_array().__repr__() for x in dfs.values()]]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1],[1,3]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,4],[1,2]],\"a:int,c:int\")\n",
    "    df3 = dag.df([[0,2],[1,1],[1,5]],\"a:int,d:int\")\n",
    "    dag.zip(df1,df2,df3).transform(to_str).show()\n",
    "    dag.zip(dict(a=df1,b=df2,c=df3)).transform(to_str).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "res:[str]                                                                                           \n",
      "----------------------------------------------------------------------------------------------------\n",
      "['[[0, 1]]', '[[0, 4]]', '[[0, 2]]']                                                                \n",
      "['[[1, 3]]', '[[1, 2]]', '[[1, 1], [1, 5]]']                                                        \n",
      "Total count: 2\n",
      "\n",
      "PandasDataFrame\n",
      "res:[str]                                                                                           \n",
      "----------------------------------------------------------------------------------------------------\n",
      "['[[0, 1]]', '[[0, 4]]', '[[0, 2]]']                                                                \n",
      "['[[1, 3]]', '[[1, 2]]', '[[1, 1], [1, 5]]']                                                        \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}