{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processor\n",
    "\n",
    "`Processor` represents the logic unit executing on driver on the **entire** input dataframes. While there is overlap with `Transformer`, transformers are more focused on the logic execution on a partition-level. \n",
    "\n",
    "In this tutorial are the methods to define a `Processor`. There is no preferred method and Fugue makes it flexible for users to choose whatever interface works for them. The four ways are native approach, schema hint, decorator, and the class interface in order of simplicity.\n",
    "\n",
    "## Example Use Cases\n",
    "\n",
    "* **Combining multiple DataFrames** into one like `concat`\n",
    "* **Column-wise aggregates on the whole DataFrame**. For example, getting the standard deviation of a column.\n",
    "* **Performing logic that requires Spark of Dask functions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Notes on Usage\n",
    "\n",
    "**ExecutionEngine aware**\n",
    "\n",
    "* Processors run on the driver so they are aware of the `ExecutionEngine` being used. Passing a parameter with the `ExecutionEngine` annotation will pass in the current `ExecutionEngine`. There is an example of this later.\n",
    "\n",
    "**Acceptable input DataFrame types**\n",
    "\n",
    "* `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "* Input can also be Fugue `DataFrames`, which is a collection of Fugue multiple `DataFrame`. \n",
    "\n",
    "**Acceptable output DataFrame types**\n",
    "\n",
    "* `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Further Notes**\n",
    "\n",
    "* If the output type is NOT one of Fugue `DataFrame`, `LocalDataFrame` or `pd.DataFrame`, the output schema must be specified because it can't be inferred.\n",
    "* `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame` or `DataFrame`\n",
    "* `DataFrame` or `DataFrames` are the recommended input/output types. All other acceptable types are variations of `LocalDataFrame`, which means that the data has to be collected on one machine (the driver) to process.\n",
    "* `Iterable`-like input may have different execution plans to bring data to driver, in some cases it can be less optimal, you must be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Approach\n",
    "\n",
    "The native approach is using a regular function without any edits beyond type annotations for both the input dataframes and output. It is converted to a Fugue extension during runtime. In the example below, we have three functions. The first one,`add1`,  has an output type of `pd.DataFrame`, which means that the schema is already known. The second one, `add`, has an output type of `Iterable[Dict[str,Any]]`, which does hold schema so it has to be provided during the `process` call inside `FugueWorkflow`.\n",
    "\n",
    "Lastly, `concat` shows how to combine multiple DataFrames into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |3    \n",
      "0    |4    \n",
      "0    |3    \n",
      "0    |4    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "# fugue knows the schema because the output in pd.DataFrame\n",
    "def add1(df:pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df[\"b\"]+=n\n",
    "    return df\n",
    "\n",
    "# schema is not known so it has to be provided later\n",
    "# in practice, it's rare to use such output type for a processor\n",
    "def add2(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2]],\"a:int,b:int\")\n",
    "    df1 = df.process(add1, params={\"n\":2})\n",
    "    df2 = df.process(add2, schema=\"a:int,b:int\", params={\"n\":2})\n",
    "    dag.process(df1,df2, using=concat).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important to know how to use `DataFrames` as input annotation. Because this is the only way accept a **dynamic** number of input DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |1    \n",
      "0    |2    \n",
      "1    |3    \n",
      "1    |1    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import DataFrames, DataFrame\n",
    "\n",
    "def concat(dfs:DataFrames) -> pd.DataFrame:\n",
    "    pdfs = [df.as_pandas() for df in dfs.values()]\n",
    "    return pd.concat(pdfs).reset_index(drop=True) # Fugue can't take pandas dataframe with special index\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,2],[1,3]],\"a:int,b:int\")\n",
    "    df3 = dag.df([[1,1]],\"a:int,b:int\")\n",
    "    dag.process(df1,df2,df3,using=concat).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Hint\n",
    "\n",
    "The schema can also be provided during the function definition through the use of the schema hint comment. Providing it during definition means it does not need to be provided inside the `FugueWorkflow`.\n",
    "\n",
    "If you are using `DataFrame`, `LocalDataFrame` or `pd.DataFrame` as the output type, schema hints can't be used because the schema will be inferred. Also, the best practice is to use `DataFrame` as the output type when using schema hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |2    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema: a:int, b:int\n",
    "def add(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1]],\"a:int,b:int\")\n",
    "    df.process(add).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "There is no obvious advantage to use the decorator approach for defining a `Processor`. In general, the decorator is good if the schema is too long to type out as a comment in one line or for adding explicitness to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |2    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import processor\n",
    "\n",
    "@processor(\"a:int, b:int\")\n",
    "def add(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").process(add).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach (Advanced)\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of use cases and are simpler to use. But if you want to get all execution context such as partition information, use interface approach.\n",
    "\n",
    "In the interface approach, type annotations are not necessary but it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |1    \n",
      "1    |1    \n",
      "1    |2    \n",
      "0    |3    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import Processor, DataFrames, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "\n",
    "class Partitioner(Processor):\n",
    "    def process(self, dfs:DataFrames) -> DataFrame:\n",
    "        assert len(dfs)==1\n",
    "        engine = self.execution_engine\n",
    "        partion = self.partition_spec\n",
    "        return engine.repartition(dfs[0], partition_spec = partion)\n",
    "\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,3],[1,2],[1,1]],\"a:int,b:int\")\n",
    "    # see the output is sorted by b, partition is passed into Partitioner as partition_spec\n",
    "    df.partition(num=1, presort=\"b\").process(Partitioner).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ExecutionEngine\n",
    "\n",
    "In some cases, the `Processor` has to be aware of the `ExecutionEngine`. **This is an example of how to write native Spark code inside Fugue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |2    \n",
      "0    |3    \n",
      "1    |4    \n",
      "1    |2    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import ExecutionEngine\n",
    "from fugue_spark import SparkDataFrame\n",
    "\n",
    "# pay attention to the input and output annotations, \n",
    "# the function uses general DataFrame instead of Spark DataFrame\n",
    "def add(e:ExecutionEngine, df:DataFrame, temp_name=\"x\") -> DataFrame:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    df = e.to_df(df) # to make sure df is SparkDataFrame, or conversion is done here\n",
    "    df.native.createOrReplaceTempView(temp_name)  # df.native is spark dataframe\n",
    "    sdf = e.spark_session.sql(\"select a,b+1 as b from \"+temp_name)  # this is how you get spark session\n",
    "    return SparkDataFrame(sdf) # you must wrap as Fugue SparkDataFrame to return\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.process(add, params={\"temp_name\":\"y\"}).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}