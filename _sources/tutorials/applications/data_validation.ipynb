{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Validation\n",
    "\n",
    "Data validation is having checks in place to make sure that data comes in the format and specifications that we expect. As data pipelines become more interconnected, the chance of changes unintentionally breaking other pipelines also increases. Validations are used to guarantee that upstream changes will not break the integrity of downstream data operations. \n",
    "\n",
    "Common data validation patterns include checking for NULL values or checking data frame shape to ensure transformations donâ€™t drop any records. Other frequently used operations are checking for column existence and schema. Using data validation avoids silent failures of data processes where everything will run successfully but provide inaccurate results. \n",
    "\n",
    "There are a couple things that using Fugue provides:\n",
    "* Allows validation code to be reused for both Pandas and Spark projects\n",
    "* Ability to use familiar Pandas-based libraries on Spark\n",
    "* Simple interface for validation on each partition of data\n",
    "\n",
    "To illustrate this, we'll use a simple example with the following Pandas DataFrame.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.DataFrame({'state': ['FL','FL','FL','CA','CA','CA'], \n",
    "                     'city': ['Orlando', 'Miami', 'Tampa',\n",
    "                              'San Francisco', 'Los Angeles', 'San Diego'],\n",
    "                     'price': [8, 12, 10, 16, 20, 18]})\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FL</td>\n",
       "      <td>Miami</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FL</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state           city  price\n",
       "0    FL        Orlando      8\n",
       "1    FL          Miami     12\n",
       "2    FL          Tampa     10\n",
       "3    CA  San Francisco     16\n",
       "4    CA    Los Angeles     20"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Pandera for Data Validation\n",
    "\n",
    "Data Validation can be placed at the start of the data pipeline to make sure that any transformations happen smoothly, and it can also be placed at the end to make sure everything is working well before output gets committed to the database. [Pandera](https://github.com/pandera-dev/pandera) is a good data validation framework because it is lightweight and has an expressive syntax. \n",
    "\n",
    "For the above DataFrame, we want to guarantee that the price is within a certain range. We want to make sure that the `price` column is at least 8 and not more than 20."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandera as pa\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=8,max_value=20)),\n",
    "})\n",
    "\n",
    "def price_validation(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    price_check.validate(data)\n",
    "    return data\n",
    "\n",
    "price_validation(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FL</td>\n",
       "      <td>Miami</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FL</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state           city  price\n",
       "0    FL        Orlando      8\n",
       "1    FL          Miami     12\n",
       "2    FL          Tampa     10\n",
       "3    CA  San Francisco     16\n",
       "4    CA    Los Angeles     20\n",
       "5    CA      San Diego     18"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the example above, we are using pandera's `DataFrameSchema` to create a validation schema. In the `price_check` variable, we have a `Check` that is applied to a `Column` named price. That validation guarantees that the prices are within an acceptable range of values. We don't need to wrap the validation inside a `price_validation` function, but this will make bring the validation to Spark seamless.\n",
    "\n",
    "We highly suggest checking the [Pandera documentation](https://pandera.readthedocs.io/en/stable/) for more information. If you want to see Pandera in action, change the `min_value` or `max_value` in the code above to trigger an error."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Pandera on Spark\n",
    "\n",
    "[Pandera](https://github.com/pandera-dev/pandera) is a great library, but unfortunately it is only avaialable in Pandas at the moment. When the data size becomes too big to handle in Pandas, users would need to switch to a data validation library with Spark support. This would involve re-implementing the same logic on Spark. Fugue, as an abstraction layer, allows users to keep using their logic written in Pandera, and port it to Spark or Dask.\n",
    "\n",
    "Below is an example of bringing Pandera validations to Spark with minimal code changes.\n",
    "\n",
    "*Note that pyspark needs to be installed in order for the code snippet below to run*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=5,max_value=20)),\n",
    "})\n",
    "\n",
    "# schema: *\n",
    "def price_validation(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    price_check.validate(data)\n",
    "    return data\n",
    "\n",
    "# Bring the code to spark\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df(data).transform(price_validation)\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SparkDataFrame\n",
      "state:str|city:str                                                                       |price:long\n",
      "---------+-------------------------------------------------------------------------------+----------\n",
      "FL       |Orlando                                                                        |8         \n",
      "FL       |Miami                                                                          |12        \n",
      "FL       |Tampa                                                                          |10        \n",
      "CA       |San Francisco                                                                  |16        \n",
      "CA       |Los Angeles                                                                    |20        \n",
      "CA       |San Diego                                                                      |18        \n",
      "Total count: 6\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There were very minimal code changes to bring the code to Spark. We only needed to add the schema hint as a comment. Fugue reads this and checks to see if the schema is upheld. If users move away from Fugue, the comment just stays as a helpful comment.\n",
    "\n",
    "The bottom section is the only addition to bring it to Spark. We use the Fugue transform method to apply the function. Because we passed `SparkExecutionEngine` to `FugueWorkflow`, this code will now run in Spark."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation by Partition with Fugue\n",
    "\n",
    "There is one current shortcoming of the current data validation frameworks. For the data we have, the price ranges of CA and FL are drastically different. Because the validation is applied per column, we donâ€™t have a way to specify different price ranges for each location. It would be ideal however if we could apply a different check for each group of data. This is what we call **validation by partition**\n",
    "\n",
    "This operation becomes very trivial to perform with Fugue. In the above example, we want to apply a different validation for the data in FL and the data in CA. On average, the CA data points have a higher price so we want to create two validation rules depending on the `state`. We do this in the code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "price_check_FL = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=7,max_value=13)),\n",
    "})\n",
    "\n",
    "price_check_CA = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=15,max_value=21)),\n",
    "})\n",
    "\n",
    "price_checks = {'CA': price_check_CA, 'FL': price_check_FL}\n",
    "\n",
    "# schema: *\n",
    "def price_validation(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    location = df['state'].iloc[0]\n",
    "    check = price_checks[location]\n",
    "    check.validate(df)\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df(data).partition(by=[\"state\"]).transform(price_validation)\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SparkDataFrame\n",
      "state:str|city:str                                                                       |price:long\n",
      "---------+-------------------------------------------------------------------------------+----------\n",
      "CA       |San Francisco                                                                  |16        \n",
      "CA       |Los Angeles                                                                    |20        \n",
      "CA       |San Diego                                                                      |18        \n",
      "FL       |Orlando                                                                        |8         \n",
      "FL       |Miami                                                                          |12        \n",
      "FL       |Tampa                                                                          |10        \n",
      "Total count: 6\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below should already look familiar by now. All we did was create two different Pandera schema objects. After that, we modified the `price_validation` to pull the location from the DataFrame and apply the approprivate validation. There are two states in our original DataFrame: CA and FL. However, when the data enters the price_validation function, it is already partitioned by the state because of the `partition(by=[\"state\"])` method call before `transform()`. This means the function is applied twice: one for FL and once for CA.\n",
    "\n",
    "Here, we are taking advantage of the SparkExecutionEngine by distributing the task across multiple partitions. We partition the data by `state`, and then apply different rules depending on the `state`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo we showed how Fugue allows Pandas-based data validation frameworks to be used in Spark. This is helpful for organizations that find themselves implementing validation rules twice to support Spark and Pandas implementations. Even though we demoed with Pandera here, this will work with other data validation libraries.\n",
    "\n",
    "Fugue also allows users to perform **validation by paritition**, a missing feature in the current data validation frameworks."
   ],
   "metadata": {}
  }
 ]
}