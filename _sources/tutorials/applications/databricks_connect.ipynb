{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using databricks-connect\n",
    "\n",
    "A lot of Databricks users use the `databricks-connect` library to execute Spark commands on a Databricks cluster instead of a local session. `databricks-connect` replaces the local installation of `pyspark` and makes `pyspark` code get executed on the cluster, allowing users to use the cluster directly from their IDE. \n",
    "\n",
    "For more information check the [Databricks](https://docs.databricks.com/dev-tools/databricks-connect.html) or [Azure](https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect) documentation for how to configure `databricks-connect`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty of Testing\n",
    "\n",
    "While `databricks-connect` is convenient for leveraging cluster compute resources, it also makes it difficult to conduct tests quickly. Spinning up a cluster takes around 3-4 minutes, slowing down iterations. Development also becomes very expensive as the `databricks-connect` configuration is attached to a cluster, meaning that it's inconvenient to switch to a smaller cluster for smaller tests.\n",
    "\n",
    "This whole process is costly in most developer time with slow iterations, and with cluster costs, having to spin up everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue and databricks-connect\n",
    "\n",
    "Fugue helps solve this problem by allowing users to use the default `NativeExecutionEngine` for local development and testing. Users can use sampled files for local development, and then bring it to Spark when ready for larger tests. To demo this, we have a sample code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "# schema: *, reversed:str\n",
    "def reverse_word(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['reversed'] = df['words'].apply(lambda x: x[::-1])\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data)\n",
    "    df = df.transform(reverse_word)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this DataFrame is just 4 rows, but we should need to bring it to the Spark cluster if we were using `databricks-connect`. Here, we perform the test locally first and confirm that it works. After that, we can bring it to Spark with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Pandas DataFrame gets converted to Spark\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df(data)\n",
    "    df = df.transform(reverse_word)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No added work is needed. The `SparkExecutionEngine` imports `pyspark`, meaning that it will import the `databricks-connect` configuration under the hood and use the configured cluster. Fugue works with `databricks-connect` seamlessly, allowing for convenient switching between local development and a remote cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added Configuration\n",
    "\n",
    "Most `databricks-connect` users add additional Spark configurations on the cluster. If additional configruation is needed in local code, it can be provided with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .config(\"fugue.dummy\",\"dummy\")\n",
    "                 .getOrCreate())\n",
    "\n",
    "engine = SparkExecutionEngine(spark_session, {\"additional_conf\":\"abc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fugue-sql on the Cluster\n",
    "\n",
    "Because Fugue-sql also just uses the `SparkExecutionEngine`, it can also be easily executed on a remote cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "fsql(\"\"\"\n",
    "      SELECT *\n",
    "        FROM data\n",
    "      TRANSFORM USING reverse_word\n",
    "      PRINT\"\"\"\n",
    ").run(SparkExecutionEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we have shown the painpoints in using `databricks-connect`. It slows down developer productitity and increases compute costs. We can solve both of these problems by toggling between Fugue's default `NativeExecutionEngine` and `SparkExecutionEngine`. Fugue's `SparkExecutionEngine` will seamlessly use whatever `pyspark` is configured for the user.\n",
    "\n",
    "Fugue also allows for additional configuration of the underlying frameworks. We showed the syntax for passing a `SparkSession` to the `SparkExecutionEngine`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}